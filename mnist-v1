"""A very simple MNIST classifier.
See extensive documentation at
https://www.tensorflow.org/get_started/mnist/beginners
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import sys

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

FLAGS = None

"""
* Tutorial for Expert :  https://www.tensorflow.org/get_started/mnist/pros
* Paper (Deep Residual Learning for Image Recognition) https://arxiv.org/pdf/1512.03385.pdf
"""

def model(X, w, w2, w3, w4, w5, w6, w_o, b1, b2, b3, p_keep_conv, p_keep_hidden):
    r1 = tf.nn.conv2d(X, w, strides=[1, 1, 1, 1], padding='SAME')
    r2 = tf.nn.relu(r1)
    r3 = tf.nn.max_pool(r2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # 14x14x1x32
    r4 = tf.nn.dropout(r3, p_keep_conv)

    r5 = tf.nn.conv2d(r4, w2, strides=[1, 1, 1, 1], padding='SAME')
    r6 = tf.nn.relu(r5)
    r7 = tf.nn.max_pool(r6, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # 7x7x1x64
    r8 = tf.nn.dropout(r7, p_keep_conv)

    r9 = tf.nn.conv2d(r8, w3, strides=[1, 1, 1, 1], padding='SAME')
    r10 = tf.nn.relu(r9)
    r11 = tf.nn.max_pool(r10, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # 4x4x1x128
    r12 = tf.nn.dropout(r11, p_keep_conv)

    r13 = tf.reshape(r12, [-1, 4 * 4 * 256])  # reshape to (?, 2048)

    y1 = tf.nn.dropout(tf.nn.relu(tf.matmul(r13, w4)+b1), p_keep_hidden)
    y2 = tf.nn.dropout(tf.nn.relu(tf.matmul(y1, w5)+b2), p_keep_hidden)
    y3 = tf.nn.dropout(tf.nn.relu(tf.matmul(y2, w6)+b3), p_keep_hidden)

    return tf.matmul(y3, w_o)


def main(_):
    # Import data
    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)
    trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels
    tf.set_random_seed(777)

    # Create the model
    x = tf.placeholder(tf.float32, [None, 784])  # 28 x 28
    sz = 4 * 4 * 256

    Z = 5

    x_reshape = tf.reshape(x, [-1, 28, 28, 1])  # x.reshape(-1, 28, 28, 1)
    W_conv1 = tf.get_variable("CW1", shape=[Z, Z, 1, 32], initializer=tf.contrib.layers.xavier_initializer_conv2d(True))
    W_conv2 = tf.get_variable("CW2", shape=[Z, Z, 32, 64], initializer=tf.contrib.layers.xavier_initializer_conv2d(True))
    W_conv3 = tf.get_variable("CW3", shape=[Z, Z, 64, 256], initializer=tf.contrib.layers.xavier_initializer_conv2d(True))
    W1 = tf.get_variable("W1", shape=[sz, 625], initializer=tf.contrib.layers.xavier_initializer(True))
    W2 = tf.get_variable("W2", shape=[625, 100], initializer=tf.contrib.layers.xavier_initializer(True))
    W3 = tf.get_variable("W3", shape=[100, 50], initializer=tf.contrib.layers.xavier_initializer(True))
    W4 = tf.get_variable("W4", shape=[50, 10], initializer=tf.contrib.layers.xavier_initializer(True))

    b1 = tf.get_variable("b1", shape=[625], initializer=tf.contrib.layers.xavier_initializer(True))
    b2 = tf.get_variable("b2", shape=[100], initializer=tf.contrib.layers.xavier_initializer(True))
    b3 = tf.get_variable("b3", shape=[50], initializer=tf.contrib.layers.xavier_initializer(True))

    p_keep_conv = tf.placeholder("float")
    p_keep_hidden = tf.placeholder("float")

    y_out = model(x_reshape, W_conv1, W_conv2, W_conv3, W1, W2, W3, W4, b1, b2, b3, p_keep_conv, p_keep_hidden)
    y_ = tf.placeholder(tf.float32, [None, 10])

    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_out))
    #train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)  # 0.9855 (batch 2)
    train_step = tf.train.AdamOptimizer(0.0005).minimize(cost)  # 0.9855 (batch 2)

    """
    batch 0
    0.9448   -> 0.9711 --> 0.9776 (Conv Filter Size 3->4)   -> 0.9863 (Larger Number of Filter) -> 0.9884 (Added bias)
    batch 1
    0.9725   -> 0.982
    batch 2
    0.9792
    batch 3
    0.9823
    batch 4
    0.9854
    batch 5
    0.9872
    batch 6
    0.9887
    batch 7
    0.9899
    batch 8
    0.9904
    batch 9
    0.99         -> 0.9931
    batch 10
    0.9909
    batch 11
    0.9919
    batch 12
    """

    sess = tf.InteractiveSession()
    tf.global_variables_initializer().run()
    # Train

    batch_size = 128

    for i in range(100):
        print("batch", i)
        training_batch = zip(range(0, len(trX), batch_size),
                             range(batch_size, len(trX) + 1, batch_size))

        for start, end in training_batch:
            sess.run(train_step, feed_dict={x: trX[start:end], y_: trY[start:end], p_keep_conv: 0.8, p_keep_hidden: 0.8})


        # Test trained model
        correct_prediction = tf.equal(tf.argmax(y_out, 1), tf.argmax(y_, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
        print(sess.run(accuracy,
                       feed_dict={x: mnist.test.images, y_: mnist.test.labels, p_keep_conv: 1.0, p_keep_hidden: 1.0}))


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', type=str, default='/tmp/tensorflow/mnist/input_data',
                        help='Directory for storing input data')
    FLAGS, unparsed = parser.parse_known_args()
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
    # max = 0.9952
